#!/usr/bin/env python3

# SPDX-FileCopyrightText: 2023 Technology Innovation Institute (TII)
#
# SPDX-License-Identifier: Apache-2.0

# pylint: disable=invalid-name, too-many-instance-attributes
# pylint: disable=too-many-arguments

""" Python script to run and summarize vulnerability scans for Ghaf """

import os
import sys
import re
import argparse
import logging
import subprocess
import tempfile
import shutil
import difflib
from pathlib import Path

import git
import pandas as pd
from colorlog import ColoredFormatter, default_log_colors
from tabulate import tabulate

###############################################################################

MYDIR_FROM_REPOROOT = "ci/vulnerability-scan"
DATADIR_FROM_REPOROOT = "ci/vulnerability-scan/data"
LOG_SPAM = logging.DEBUG - 1
LOG = logging.getLogger(os.path.abspath(__file__))

###############################################################################


def getargs():
    """Parse command line arguments"""
    desc = "Helper script to run and summarize vulnerability scans."
    epil = "Example: ghafscan /path/to/ghaf/repo"
    parser = argparse.ArgumentParser(description=desc, epilog=epil)
    helps = (
        "Path to target Ghaf git repository. Script will search for a valid "
        "git repository from the given path or any of its parent directories."
    )
    parser.add_argument("TARGET", help=helps, type=Path)
    helps = "Set the debug verbosity level between 0-3 (default: --verbose=1)"
    parser.add_argument("--verbose", help=helps, type=int, default=1)
    return parser.parse_args()


################################################################################


# Utils


def set_log_verbosity(verbosity=1):
    """Set logging verbosity"""
    log_levels = [logging.NOTSET, logging.INFO, logging.DEBUG, LOG_SPAM]
    verbosity = min(len(log_levels) - 1, max(verbosity, 0))
    _init_logging(verbosity)


def _init_logging(verbosity=1):
    """Initialize logging"""
    if verbosity == 0:
        level = logging.NOTSET
    elif verbosity == 1:
        level = logging.INFO
    elif verbosity == 2:
        level = logging.DEBUG
    else:
        level = LOG_SPAM
    if level <= logging.DEBUG:
        logformat = (
            "%(log_color)s%(levelname)-8s%(reset)s "
            "%(filename)s:%(funcName)s():%(lineno)d "
            "%(message)s"
        )
    else:
        logformat = "%(log_color)s%(levelname)-8s%(reset)s %(message)s"
    logging.addLevelName(LOG_SPAM, "SPAM")
    default_log_colors["INFO"] = "fg_bold_white"
    default_log_colors["DEBUG"] = "fg_bold_white"
    default_log_colors["SPAM"] = "fg_bold_white"
    formatter = ColoredFormatter(logformat, log_colors=default_log_colors)
    if LOG.hasHandlers() and len(LOG.handlers) > 0:
        stream = LOG.handlers[0]
    else:
        stream = logging.StreamHandler()
    stream.setFormatter(formatter)
    if not LOG.hasHandlers():
        LOG.addHandler(stream)
    LOG.setLevel(level)


def exit_unless_command_exists(name):
    """Check if `name` is an executable in PATH"""
    name_is_in_path = shutil.which(name) is not None
    if not name_is_in_path:
        LOG.fatal("command '%s' is not in PATH", name)
        sys.exit(1)


def exec_cmd(cmd, raise_on_error=True, return_error=False, loglevel=logging.DEBUG):
    """Run shell command cmd"""
    command_str = " ".join(cmd)
    LOG.log(loglevel, "Running: %s", command_str)
    try:
        ret = subprocess.run(cmd, capture_output=True, encoding="utf-8", check=True)
        return ret
    except subprocess.CalledProcessError as error:
        LOG.debug(
            "Error running shell command:\n cmd:   '%s'\n stdout: %s\n stderr: %s",
            command_str,
            error.stdout,
            error.stderr,
        )
        if raise_on_error:
            raise error
        if return_error:
            return error
        return None


def df_from_csv_file(name):
    """Read csv file into dataframe"""
    LOG.debug("Reading: %s", name)
    try:
        df = pd.read_csv(name, keep_default_na=False, dtype=str)
        df.reset_index(drop=True, inplace=True)
        return df
    except pd.errors.ParserError:
        LOG.fatal("Not a csv file: '%s'", name)
        sys.exit(1)


def df_log(df, loglevel, tablefmt="presto"):
    """Log dataframe with given loglevel and tablefmt"""
    if LOG.level <= loglevel:
        if df.empty:
            return
        df = df.fillna("")
        table = tabulate(
            df, headers="keys", tablefmt=tablefmt, stralign="left", showindex=False
        )
        LOG.log(loglevel, "\n%s\n", table)


def filediff(file1, file2):
    """Return unified diff between `file1` and `file2` as a string"""
    f1 = Path(file1)
    f2 = Path(file2)
    if not f1.exists():
        LOG.error("Diff failed: '%s' does not exist", str(f1))
        return ""
    if not f2.exists():
        LOG.error("Diff failed: '%s' does not exist", str(f2))
        return ""
    f1_lines = f1.read_text(encoding="utf-8").splitlines()
    f2_lines = f2.read_text(encoding="utf-8").splitlines()
    diff = difflib.unified_diff(f1_lines, f2_lines, fromfile=file1, tofile=file2)
    return "\n".join(diff).strip(" \n\t")


###############################################################################


class GhafScanner:
    """Scan vulnerabilities in Ghaf repo"""

    def __init__(self, ghaf_path):
        self.lockfile = None
        self.lockfile_bak = None
        self.flake = None
        self.flake_bak = None
        self.tmpdir = None
        ghaf_path_abs = Path(ghaf_path).resolve().as_posix()
        LOG.debug("Finding repo based on file path: %s", ghaf_path_abs)
        self.repo = git.Repo(ghaf_path_abs, search_parent_directories=True)
        LOG.info("Repo HEAD at: %s", self.repo.rev_parse("HEAD"))
        self.reporoot = Path(self.repo.working_tree_dir)
        self.tmpdir = Path(tempfile.mkdtemp())
        LOG.debug("Using tmpdir: %s", self.tmpdir)
        # Backup the original lockfile
        self.lockfile = self.reporoot / "flake.lock"
        if not self.lockfile.exists():
            LOG.fatal("Missing lockfile: %s", self.lockfile.resolve())
            sys.exit(1)
        self.lockfile_bak = self.tmpdir / "flake.lock"
        LOG.debug("%s:\n%s", self.lockfile, self.lockfile.read_text())
        shutil.copy(self.lockfile, self.lockfile_bak)
        # Backup the original flake.nix
        self.flake = self.reporoot / "flake.nix"
        if not self.flake.exists():
            LOG.fatal("Missing flake file: %s", self.flake.resolve())
            sys.exit(1)
        self.flake_bak = self.tmpdir / "flake.nix"
        shutil.copy(self.flake, self.flake_bak)
        # Init datadir and files
        self.mydir = self.reporoot / MYDIR_FROM_REPOROOT
        self.datadir = self.reporoot / DATADIR_FROM_REPOROOT
        self.datadir.mkdir(parents=True, exist_ok=True)
        LOG.debug("Using datadir: %s", self.datadir)
        # Report template
        template_report = self.datadir / "summary_template.md"
        if not template_report.exists():
            LOG.fatal("Missing report template: %s", template_report)
            sys.exit(1)
        self.report_template_str = template_report.read_text()
        # Whitelist
        self.whitelist = self.mydir / "whitelist.csv"
        if not self.whitelist.exists():
            LOG.warning("Whitelist not found, ignoring")
            self.whitelist = None

    def __del__(self):
        if self.flake and self.flake_bak:
            LOG.debug("Restoring original flake.nix")
            shutil.copy(self.flake_bak, self.flake)
        if self.lockfile and self.lockfile_bak:
            LOG.debug("Restoring original lockfile")
            shutil.copy(self.lockfile_bak, self.lockfile)
        if self.tmpdir:
            LOG.debug("Removing tmpdir: %s", self.tmpdir)
            shutil.rmtree(self.tmpdir)

    def scan_target(self, target):
        """Scan given flake target"""
        LOG.info("Scanning flake target '%s'", target)
        # Store new scan results to this file
        out_new = self.datadir / f"vulns_new__{target}.csv"
        # Backup earlier scan results for reference
        out_old = self.datadir / f"vulns_old__{target}.csv"
        if out_new.exists():
            shutil.copy(out_new, out_old)

        # First scan:
        # Before lockfile update
        LOG.info("Scanning current vulnerabilities")
        drv_path = self._evaluate_target_drv(target)
        cmd_common = "nix_secupdates.py --buildtime"
        if self.whitelist:
            cmd_common += f" --whitelist={self.whitelist}"
        cmd = f"{cmd_common} --out={str(out_new)} {str(drv_path)}"
        ret = exec_cmd(cmd.split())
        LOG.debug("nix_secupdates.py ==>\n%s\n<== nix_secupdates.py\n", ret.stderr)
        self._debug_print_vulnxscan_output(ret.stderr)

        # Update lockfile to get latest updates from the pinned channel
        self._update_ghaf_lock()

        # Second scan:
        # Including latest updates from the pinned channel
        LOG.info("Scanning vulnerabilities after lockfile update")
        # Re-evaluate drv after lockfile update
        drv_path = self._evaluate_target_drv(target)
        out_upd = self.datadir / f"vulns_new_lock_updated__{target}.csv"
        cmd = f"{cmd_common} --out={str(out_upd)} {str(drv_path)}"
        ret = exec_cmd(cmd.split())
        LOG.debug("nix_secupdates.py ==>\n%s\n<== nix_secupdates.py\n", ret.stderr)
        self._debug_print_vulnxscan_output(ret.stderr)

        # Update lockfile to get latest updates from nixos-unstable
        self._update_ghaf_lock(nixpkgs_url="github:NixOS/nixpkgs/nixos-unstable")

        # Third scan:
        # Including latest updates from the nixos-unstable
        LOG.info("Scanning vulnerabilities after updating from nixos-unstable")
        # Re-evaluate drv after lockfile update
        drv_path = self._evaluate_target_drv(target)
        out_uns = self.datadir / f"vulns_new_nixos_unstable__{target}.csv"
        cmd = f"{cmd_common} --out={str(out_uns)} {str(drv_path)}"
        ret = exec_cmd(cmd.split())
        LOG.debug("nix_secupdates.py ==>\n%s\n<== nix_secupdates.py\n", ret.stderr)
        self._debug_print_vulnxscan_output(ret.stderr)

        # Generate report based on the scan outputs
        self._report_target(target, out_new, out_old, out_upd, out_uns)

    def _report_target(self, target, out_new, out_old, out_upd, out_uns):
        LOG.debug("")
        target_report = self.mydir / f"report_{target}.md"

        # Generate the report contents
        report_str = self.report_template_str

        marker = "TARGET_NAME"
        newstr = target
        report_str = report_str.replace(marker, newstr)

        marker = "FIXED_IN_NIXPKGS"
        newstr = self._df_to_report_tbl(self._csvdiff(out_new, out_upd, "left_only"))
        report_str = report_str.replace(marker, newstr)

        marker = "FIXED_IN_NIX_UNSTABLE"
        newstr = self._df_to_report_tbl(self._csvdiff(out_upd, out_uns, "left_only"))
        report_str = report_str.replace(marker, newstr)

        marker = "NEW_SINCE_LAST_RUN"
        newstr = self._df_to_report_tbl(self._csvdiff(out_new, out_old, "left_only"))
        report_str = report_str.replace(marker, newstr)

        marker = "GHAF_CURRENT_VULNS"
        newstr = self._df_to_report_tbl(df_from_csv_file(out_new))
        report_str = report_str.replace(marker, newstr)

        # Write the target report
        target_report.write_text(report_str)
        # For now, we just copy the target report to README.md.
        # When more targets are added, we need to aggregate the contents of the
        # various target reports to the main README.md, or maybe just provide
        # links to each target-specific report in the README.md
        readme = self.mydir / "README.md"
        shutil.copy(target_report, readme)

    def _debug_print_vulnxscan_output(self, nix_secupdates_logs):
        pattern = r"vulnxscan result[^']+'(/[^']+.csv)'"
        m = re.search(pattern, nix_secupdates_logs)
        if not m:
            LOG.warning("No vulnxscan output file found")
            return
        vulnxscan_csv = m.group(1)
        LOG.debug("Found vulnxscan csv: %s", vulnxscan_csv)
        df = df_from_csv_file(vulnxscan_csv)
        # Tidy the output
        df = df.drop(["version", "sortcol", "whitelist_comment"], axis=1)
        df = df.drop_duplicates(keep="first")
        df_log(df, logging.DEBUG)

    def _csvdiff(self, csv_left, csv_right, diff_filter):
        LOG.debug("")
        out = self.tmpdir / "csvdiff.csv"
        uids = "vuln_id,package"
        left = csv_left.resolve().as_posix()
        right = csv_right.resolve().as_posix()
        cmd = ["csvdiff", left, right, f"--cols={uids}", f"--out={out}"]
        exec_cmd(cmd, raise_on_error=False)
        df = df_from_csv_file(out)
        return df[df["diff"].str.contains(diff_filter)]

    def _df_to_report_tbl(self, df):
        LOG.debug("")
        # Sort by the following columns
        sort_cols = ["sortcol", "package", "version_local"]
        df = df.sort_values(by=sort_cols, ascending=False)
        # Truncate version strings
        df["version_local"] = df["version_local"].str.slice(0, 16)
        # Keep only the following columns
        report_cols = ["vuln_id", "package", "url", "version_local"]
        df = df[report_cols]
        df = df.drop_duplicates(keep="first")
        df.fillna("")
        # Format dataframe to markdown table
        table = tabulate(
            df, headers="keys", tablefmt="github", stralign="left", showindex=False
        )
        return f"\n{table}\n"

    def _evaluate_target_drv(self, target):
        cmd = ["nix", "eval", f"{str(self.reporoot)}#{target}.drvPath"]
        ret = exec_cmd(cmd)
        drv_path = Path(str(ret.stdout).strip('"\n\t '))
        LOG.info("Target '%s' evaluates to derivation: %s", target, drv_path)
        return drv_path

    def _update_ghaf_lock(self, nixpkgs_url=None):
        LOG.info("Updating: %s", self.lockfile)
        # Reset possible earlier changes to the flake.nix and lockfile
        shutil.copy(self.lockfile_bak, self.lockfile)
        shutil.copy(self.flake_bak, self.flake)
        if nixpkgs_url:
            # Update the nixpkgs.url reference in flake.nix
            flake_text = self.flake.read_text()
            # Match e.g. 'nixpkgs.url = "github:nixos/nixpkgs/nixos-23.05";'
            pattern = r"(nixpkgs\.url *= *)[^;]+"
            repl = rf'\1"{nixpkgs_url}"'
            # Replace pattern with repl in flake_text
            flake_text_new = re.sub(pattern, repl, flake_text)
            self.flake.write_text(flake_text_new)
            diffstr = filediff(str(self.flake_bak), str(self.flake))
            if not diffstr:
                LOG.warning("Replacing nixpkgs.url in %s failed", self.flake)
                LOG.debug("%s contents:\n%s", self.flake, flake_text)
        # Update the lockfile
        cmd = f"nix flake lock {str(self.reporoot)} --update-input nixpkgs"
        exec_cmd(cmd.split())
        diffstr = filediff(str(self.lockfile_bak), str(self.lockfile))
        if diffstr:
            LOG.info("Updated lockfile:\n%s", diffstr)


################################################################################


def main():
    """main entry point"""
    args = getargs()
    set_log_verbosity(args.verbose)
    # Fail early if following commands are not in PATH
    exit_unless_command_exists("git")
    exit_unless_command_exists("nix_secupdates.py")
    exit_unless_command_exists("csvdiff")
    # Scan the following targets
    scanner = GhafScanner(args.TARGET)
    scanner.scan_target("generic-x86_64-release")


if __name__ == "__main__":
    main()

################################################################################
